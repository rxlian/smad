{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "from argparse import Namespace\n",
    "import seaborn as sns\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data.dataset import random_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import cross_val_score, learning_curve, cross_val_predict, KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    variable = 'label', ##this is the label name(the label column name in .csv)\n",
    "    batch_size = 16, #batch size, we always choose 16, 32, 64 depending on the GPU memory size\n",
    "    epochs = 2, #epochs, 2, 3, or 4 epochs are always enough to train BERT\n",
    "    max_length = 128, #sequence max length, the BERT limitation is 512, we always choose 2^n\n",
    "    random_state = 42, #random_state\n",
    "    test_size = 0.2, # test set size\n",
    "    learning_rate = 5e-5, # learning rate during training BERT, we always choose 2e-5, 3e-5, 5e-5\n",
    "#     cv_num = 5, # n-fold cross-validation\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('round3/sideEffect_for_training.csv', encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following chunk is to under-sample the majority class by randomly selecting samples, \n",
    "#and make the dataset balanced.\n",
    "def undersample(dataframe, binary):\n",
    "    countdict = dataframe[binary].value_counts().to_dict()\n",
    "    count0, count1 = countdict[0], countdict[1]\n",
    "    class1 = dataframe[dataframe[binary] == 1]\n",
    "    class0 = dataframe[dataframe[binary] == 0]\n",
    "    if count1 > count0:\n",
    "        class1_under = class1.sample(count0, random_state = args.random_state) #random_state\n",
    "        newalldf = pd.concat([class1_under, class0], axis = 0, sort = False).reset_index(drop = True)\n",
    "    else:\n",
    "        class0_under = class0.sample(count1, random_state = args.random_state)  #random_state\n",
    "        newalldf = pd.concat([class0_under, class1], axis = 0, sort = False).reset_index(drop = True)\n",
    "    return newalldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "underdf = undersample(dfpre, args.variable)\n",
    "print(underdf[args.variable].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train, and test\n",
    "negdf = underdf[underdf[args.variable] == 0].reset_index(drop = True)\n",
    "posdf = underdf[underdf[args.variable] == 1].reset_index(drop = True)\n",
    "def split_train_test(posdf, negdf):\n",
    "    postrain, postest = train_test_split(posdf, test_size = args.test_size, shuffle = True, random_state = args.random_state)\n",
    "    negtrain, negtest = train_test_split(negdf, test_size = args.test_size, shuffle = True, random_state = args.random_state)\n",
    "    testdf = pd.concat([postest, negtest], axis = 0, sort = False).reset_index(drop = True)\n",
    "    traindf = pd.concat([postrain, negtrain], axis = 0, sort = False).reset_index(drop = True)\n",
    "    return traindf, testdf\n",
    "\n",
    "train, test = split_train_test(posdf, negdf)\n",
    "print(train[args.variable].value_counts())\n",
    "print(test[args.variable].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis = 1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def _init_fn(seed = 42):\n",
    "    np.random.seed(int(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###training\n",
    "def trainbert(sentences, labels):\n",
    "    training_input_ids = []\n",
    "    training_attention_masks = []\n",
    "    for sent in sentences:\n",
    "        training_encoded_dict = tokenizer.encode_plus(\n",
    "                            str(sent),\n",
    "                            add_spicial_tokens = True,\n",
    "                            max_length = args.max_length, #512\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,\n",
    "                            return_tensors = 'pt',\n",
    "\n",
    "        )\n",
    "        training_input_ids.append(training_encoded_dict['input_ids'])\n",
    "        training_attention_masks.append(training_encoded_dict['attention_mask'])\n",
    "    training_input_ids = torch.cat(training_input_ids, axis = 0)\n",
    "    training_attention_masks = torch.cat(training_attention_masks, axis = 0)\n",
    "    training_labels = torch.from_numpy(labels).long()\n",
    "\n",
    "    train_dataset = Data.TensorDataset(training_input_ids, training_attention_masks, training_labels)\n",
    "\n",
    "    train_dataloader = Data.DataLoader(\n",
    "                train_dataset,\n",
    "                sampler = Data.RandomSampler(train_dataset),\n",
    "                batch_size = args.batch_size,\n",
    "                worker_init_fn = _init_fn\n",
    "    )\n",
    "\n",
    "    total_steps = len(train_dataloader) * args.epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps                                            \n",
    "                                           )\n",
    "\n",
    "    for i in range(0, args.epochs):\n",
    "        print('Epoch {:} / {:}'.format(i + 1, args.epochs))\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            model.zero_grad()\n",
    "            loss, logits = model(b_input_ids,\n",
    "                                 token_type_ids = None,\n",
    "                                 attention_mask = b_input_mask,\n",
    "                                 labels = b_labels                            \n",
    "                                )\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(\"average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "    return avg_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####test\n",
    "def testacc(testdf):\n",
    "    testsen = testdf['tweet'].values\n",
    "    testlabels = testdf[args.variable].values\n",
    "    testinput_ids = []\n",
    "    testattention_masks = []\n",
    "    for sent in testsen:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                          sent,\n",
    "                          add_special_tokens = True,\n",
    "                          max_length = args.max_length,\n",
    "                          pad_to_max_length = True,\n",
    "                          return_attention_mask = True,\n",
    "                          return_tensors = 'pt'\n",
    "      )\n",
    "        testinput_ids.append(encoded_dict['input_ids'])\n",
    "        testattention_masks.append(encoded_dict['attention_mask'])\n",
    "    testinput_ids = torch.cat(testinput_ids, dim = 0)\n",
    "    testattention_masks = torch.cat(testattention_masks, dim = 0)\n",
    "    testlabels = torch.tensor(testlabels).long()\n",
    "    testdata = Data.TensorDataset(testinput_ids, testattention_masks, testlabels)\n",
    "    testdataloader = Data.DataLoader(\n",
    "                testdata,\n",
    "                sampler = Data.SequentialSampler(testdata),\n",
    "                batch_size = args.batch_size,\n",
    "                worker_init_fn = _init_fn\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    \n",
    "    test_acc = 0\n",
    "    test_loss = 0\n",
    "    test_steps = 0\n",
    "    for batch in testdataloader:\n",
    "        test_input_ids = batch[0].to(device)\n",
    "        test_input_mask = batch[1].to(device)\n",
    "        test_labels = batch[2].to(device)\n",
    "        with torch.no_grad():\n",
    "            (loss, logits) = model(\n",
    "                               test_input_ids,\n",
    "                               token_type_ids = None,\n",
    "                               attention_mask = test_input_mask,\n",
    "                               labels = test_labels\n",
    "\n",
    "            )\n",
    "        all_logits.append(logits)\n",
    "        test_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        testlabel_ids = test_labels.detach().cpu().numpy()\n",
    "        test_acc += flat_accuracy(logits, testlabel_ids)\n",
    "    \n",
    "    all_logits = torch.cat(all_logits, dim = 0)\n",
    "    probs = F.softmax(all_logits, dim = 1).cpu().numpy()\n",
    "    avg_testacc = test_acc / len(testdataloader)\n",
    "    return avg_testacc, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "seed_torch()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True) \n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels = 2 #num_labels = 2, binary classification\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                lr = args.learning_rate\n",
    ")\n",
    "\n",
    "trainloss = trainbert(train['tweet'].values, train[args.variable].values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_roc(probs, y_true):\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    print('acc on test set: {0:.2f}'.format(accuracy))\n",
    "    print('auc: {0:.2f}'.format(roc_auc))\n",
    "    return fpr, tpr, roc_auc, accuracy, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(fpr, tpr, roc_auc):\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "#     plt.savefig('rocprovax.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(labels, preds):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': round(acc, 2),\n",
    "        'f1': round(f1, 2),\n",
    "        'precision': round(precision, 2),\n",
    "        'recall': round(recall, 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get test result\n",
    "testacc, probs = testacc(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get test result, such as accuracy, precision, recall, f-1\n",
    "fpr, tpr, roc_auc, accuracy, y_pred = evaluate_roc(probs, test[args.variable].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(test[args.variable].values, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot ROC curve\n",
    "plot_roc(fpr, tpr, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##save trained model\n",
    "output_dir = 'models/round3/finetune_bert_liberty_under/'\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following chunk is to train BERT with 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trainbert(sentences, labels):\n",
    "#     splits = list(KFold(n_splits = args.cv_num, shuffle = True, random_state = args.random_state).split(sentences, labels)) #, random_state = 42\n",
    "#     avg_train_losses = []\n",
    "#     avg_val_losses = []\n",
    "#     avg_val_acc = []\n",
    "    \n",
    "#     whole_train_stats = []\n",
    "#     for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "#         seed_torch()\n",
    "\n",
    "#         tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
    "\n",
    "#         model = BertForSequenceClassification.from_pretrained(\n",
    "#             'bert-base-uncased',\n",
    "\n",
    "#             num_labels = 2\n",
    "#         )\n",
    "#         model.to(device)\n",
    "\n",
    "\n",
    "#         optimizer = AdamW(model.parameters(),\n",
    "#                           lr = 2e-5,\n",
    "#                           eps = 1e-8\n",
    "#         )\n",
    "        \n",
    "#         training_input_ids = []\n",
    "#         training_attention_masks = []\n",
    "#         for sent in sentences[train_idx]:\n",
    "#             training_encoded_dict = tokenizer.encode_plus(\n",
    "#                                 sent,\n",
    "#                                 add_spicial_tokens = True,\n",
    "#                                 max_length = args.max_length, #512\n",
    "#                                 pad_to_max_length = True,\n",
    "#                                 return_attention_mask = True,\n",
    "#                                 return_tensors = 'pt',\n",
    "\n",
    "#             )\n",
    "#             training_input_ids.append(training_encoded_dict['input_ids'])\n",
    "#             training_attention_masks.append(training_encoded_dict['attention_mask'])\n",
    "#         training_input_ids = torch.cat(training_input_ids, axis = 0)\n",
    "#         training_attention_masks = torch.cat(training_attention_masks, axis = 0)\n",
    "#         training_labels = torch.from_numpy(labels[train_idx])\n",
    "\n",
    "\n",
    "#         valid_input_ids = []\n",
    "#         valid_attention_masks = []\n",
    "#         for sent in sentences[valid_idx]:\n",
    "#             valid_encoded_dict = tokenizer.encode_plus(\n",
    "#                                 sent,\n",
    "#                                 add_spicial_tokens = True,\n",
    "#                                 max_length = args.max_length, #512\n",
    "#                                 pad_to_max_length = True,\n",
    "#                                 return_attention_mask = True,\n",
    "#                                 return_tensors = 'pt',\n",
    "\n",
    "#             )\n",
    "#             valid_input_ids.append(valid_encoded_dict['input_ids'])\n",
    "#             valid_attention_masks.append(valid_encoded_dict['attention_mask'])\n",
    "#         valid_input_ids = torch.cat(valid_input_ids, axis = 0)\n",
    "#         valid_attention_masks = torch.cat(valid_attention_masks, axis = 0)\n",
    "#         valid_labels = torch.from_numpy(labels[valid_idx])\n",
    "\n",
    "#         train_dataset = Data.TensorDataset(training_input_ids, training_attention_masks, training_labels)\n",
    "#         valid_dataset = Data.TensorDataset(valid_input_ids, valid_attention_masks, valid_labels)\n",
    "\n",
    "\n",
    "#         train_dataloader = Data.DataLoader(\n",
    "#                     train_dataset,\n",
    "#                     sampler = Data.RandomSampler(train_dataset),\n",
    "#                     batch_size = args.batch_size,\n",
    "#                     worker_init_fn = _init_fn\n",
    "#         )\n",
    "\n",
    "#         validation_dataloader = Data.DataLoader(\n",
    "#                     valid_dataset,\n",
    "#                     sampler = Data.SequentialSampler(valid_dataset),\n",
    "#                     batch_size = args.batch_size, \n",
    "#                     worker_init_fn = _init_fn\n",
    "#         )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         total_steps = len(train_dataloader) * args.epochs\n",
    "#         scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "#                                                 num_warmup_steps = 0,\n",
    "#                                                 num_training_steps = total_steps                                            \n",
    "#                                                )\n",
    "\n",
    "#         print(f'Fold{i + 1}')\n",
    "\n",
    "#         training_stats = []\n",
    "#         for i in range(0, args.epochs):\n",
    "#             print('Epoch {:} / {:}'.format(i + 1, args.epochs))\n",
    "#             total_train_loss = 0\n",
    "#             model.train()\n",
    "#             for step, batch in enumerate(train_dataloader):\n",
    "#                 b_input_ids = batch[0].to(device)\n",
    "#                 b_input_mask = batch[1].to(device)\n",
    "#                 b_labels = batch[2].to(device)\n",
    "\n",
    "#                 model.zero_grad()\n",
    "\n",
    "#                 loss, logits = model(b_input_ids,\n",
    "#                                      token_type_ids = None,\n",
    "#                                      attention_mask = b_input_mask,\n",
    "#                                      labels = b_labels                            \n",
    "#                                     )\n",
    "#                 total_train_loss += loss.item()\n",
    "#                 loss.backward()\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#                 optimizer.step()\n",
    "#                 scheduler.step()\n",
    "#             avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "#             print(\"average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "#             print(\"running validation\")\n",
    "#             model.eval()\n",
    "            \n",
    "#             total_eval_accuracy = 0\n",
    "#             total_eval_loss = 0\n",
    "#             nb_eval_steps = 0\n",
    "#             for batch in validation_dataloader:\n",
    "#                 val_input_ids = batch[0].to(device)\n",
    "#                 val_input_mask = batch[1].to(device)\n",
    "#                 val_labels = batch[2].to(device)\n",
    "#                 with torch.no_grad():\n",
    "#                     (valloss, vallogits) = model(val_input_ids,\n",
    "#                                            token_type_ids = None,\n",
    "#                                            attention_mask = val_input_mask,\n",
    "#                                            labels = val_labels                              \n",
    "#                                           )\n",
    "#                 total_eval_loss += valloss.item()\n",
    "#                 vallogits = vallogits.detach().cpu().numpy()\n",
    "#                 label_ids = val_labels.to('cpu').numpy()\n",
    "#                 total_eval_accuracy += flat_accuracy(vallogits, label_ids)\n",
    "            \n",
    "#             avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "#             print(\"accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "#             avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "#             print(\"average val loss: {0:.2f}\".format(avg_val_loss))\n",
    "            \n",
    "#             avg_train_losses.append(avg_train_loss)\n",
    "#             avg_val_losses.append(avg_val_loss)\n",
    "#             avg_val_acc.append(avg_val_accuracy)\n",
    "            \n",
    "\n",
    "#             training_stats.append(\n",
    "#             {\n",
    "#                 'epoch': i + 1,\n",
    "#                 'training loss': avg_train_loss,\n",
    "#                 'valid loss': avg_val_loss,\n",
    "#                 'valid acc': avg_val_accuracy\n",
    "#             }\n",
    "#             )\n",
    "            \n",
    "        \n",
    "#         whole_train_stats.append(training_stats)\n",
    "\n",
    "#     print('All \\t train_loss={:.2f} \\t val_loss={:.2f} \\t val_acc={:.2f}'.format(np.mean(avg_train_losses), np.mean(avg_val_losses), np.mean(avg_val_acc)))\n",
    "#     return whole_train_stats, avg_train_losses, avg_val_losses, avg_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is to further split the training set into trainining and validation (20%). And then train and validate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trainbert(sentences, labels):\n",
    "#     xtrain, xtest, ytrain, ytest = train_test_split(sentences, labels, test_size=0.2, shuffle = True,\n",
    "#                                                         random_state=42)\n",
    "#     avg_train_losses = []\n",
    "#     avg_val_losses = []\n",
    "#     avg_val_acc = []\n",
    "#     training_input_ids = []\n",
    "#     training_attention_masks = []\n",
    "#     training_stats = []\n",
    "#     for sent in xtrain:\n",
    "#         training_encoded_dict = tokenizer.encode_plus(\n",
    "#                             sent,\n",
    "#                             add_spicial_tokens = True,\n",
    "#                             max_length = args.max_length, #512\n",
    "#                             pad_to_max_length = True,\n",
    "#                             return_attention_mask = True,\n",
    "#                             return_tensors = 'pt',\n",
    "\n",
    "#         )\n",
    "#         training_input_ids.append(training_encoded_dict['input_ids'])\n",
    "#         training_attention_masks.append(training_encoded_dict['attention_mask'])\n",
    "#     training_input_ids = torch.cat(training_input_ids, axis = 0)\n",
    "#     training_attention_masks = torch.cat(training_attention_masks, axis = 0)\n",
    "#     training_labels = torch.from_numpy(ytrain).long()\n",
    "\n",
    "#     train_dataset = Data.TensorDataset(training_input_ids, training_attention_masks, training_labels)\n",
    "\n",
    "#     train_dataloader = Data.DataLoader(\n",
    "#                 train_dataset,\n",
    "#                 sampler = Data.RandomSampler(train_dataset),\n",
    "#                 batch_size = args.batch_size,\n",
    "#                 worker_init_fn = _init_fn\n",
    "#     )\n",
    "    \n",
    "#     valid_input_ids = []\n",
    "#     valid_attention_masks = []\n",
    "#     for valsent in xtest:\n",
    "#         val_encoded_dict = tokenizer.encode_plus(\n",
    "#                             valsent,\n",
    "#                             add_spicial_tokens = True,\n",
    "#                             max_length = args.max_length, #512\n",
    "#                             pad_to_max_length = True,\n",
    "#                             return_attention_mask = True,\n",
    "#                             return_tensors = 'pt',\n",
    "\n",
    "#         )\n",
    "#         valid_input_ids.append(val_encoded_dict['input_ids'])\n",
    "#         valid_attention_masks.append(val_encoded_dict['attention_mask'])\n",
    "#     valid_input_ids = torch.cat(valid_input_ids, axis = 0)\n",
    "#     valid_attention_masks = torch.cat(valid_attention_masks, axis = 0)\n",
    "#     val_labels = torch.from_numpy(ytest).long()\n",
    "\n",
    "#     val_dataset = Data.TensorDataset(valid_input_ids, valid_attention_masks, val_labels)\n",
    "\n",
    "#     val_dataloader = Data.DataLoader(\n",
    "#                 val_dataset,\n",
    "#                 sampler = Data.RandomSampler(val_dataset),\n",
    "#                 batch_size = args.batch_size,\n",
    "#                 worker_init_fn = _init_fn\n",
    "#     )\n",
    "\n",
    "#     total_steps = len(train_dataloader) * args.epochs\n",
    "#     scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "#                                             num_warmup_steps = 0,\n",
    "#                                             num_training_steps = total_steps                                            \n",
    "#                                            )\n",
    "\n",
    "#     for i in range(0, args.epochs):\n",
    "#         print('Epoch {:} / {:}'.format(i + 1, args.epochs))\n",
    "#         total_train_loss = 0\n",
    "#         model.train()\n",
    "#         for step, batch in enumerate(train_dataloader):\n",
    "#             b_input_ids = batch[0].to(device)\n",
    "#             b_input_mask = batch[1].to(device)\n",
    "#             b_labels = batch[2].to(device)\n",
    "#             model.zero_grad()\n",
    "#             loss, logits = model(b_input_ids,\n",
    "#                                  token_type_ids = None,\n",
    "#                                  attention_mask = b_input_mask,\n",
    "#                                  labels = b_labels                            \n",
    "#                                 )\n",
    "#             total_train_loss += loss.item()\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#             optimizer.step()\n",
    "#             scheduler.step()\n",
    "#         avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "#         print(\"average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        \n",
    "#         print(\"running validation\")\n",
    "#         model.eval()\n",
    "#         total_eval_accuracy = 0\n",
    "#         total_eval_loss = 0\n",
    "#         nb_eval_steps = 0\n",
    "#         for batch in val_dataloader:\n",
    "#             val_input_ids = batch[0].to(device)\n",
    "#             val_input_mask = batch[1].to(device)\n",
    "#             val_labels = batch[2].to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 (valloss, vallogits) = model(val_input_ids,\n",
    "#                                             token_type_ids = None,\n",
    "#                                             attention_mask = val_input_mask,\n",
    "#                                             labels = val_labels                              \n",
    "#                                         )\n",
    "#             total_eval_loss += valloss.item()\n",
    "#             vallogits = vallogits.detach().cpu().numpy()\n",
    "#             label_ids = val_labels.to('cpu').numpy()\n",
    "#             total_eval_accuracy += flat_accuracy(vallogits, label_ids)\n",
    "            \n",
    "#         avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
    "#         print(\"accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "#         avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "#         print(\"average val loss: {0:.2f}\".format(avg_val_loss))\n",
    "            \n",
    "#         avg_train_losses.append(avg_train_loss)\n",
    "#         avg_val_losses.append(avg_val_loss)\n",
    "#         avg_val_acc.append(avg_val_accuracy)\n",
    "\n",
    "\n",
    "#         training_stats.append(\n",
    "#         {\n",
    "#             'epoch': i + 1,\n",
    "#             'training loss': avg_train_loss,\n",
    "#             'valid loss': avg_val_loss,\n",
    "#             'valid acc': avg_val_accuracy\n",
    "#         }\n",
    "#         )\n",
    "            \n",
    "#     print('All \\t train_loss={:.2f} \\t val_loss={:.2f} \\t val_acc={:.2f}'.format(np.mean(avg_train_losses), np.mean(avg_val_losses), np.mean(avg_val_acc)))\n",
    "# #     return whole_train_stats, avg_train_losses, avg_val_losses, avg_val_acc\n",
    "\n",
    "#     return avg_train_loss, training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###plot the learning curve, only can be plot if trained with cross-validation or validation set\n",
    "# pd.set_option('precision', 2)\n",
    "\n",
    "# # Create a DataFrame from our training statistics.\n",
    "# df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# # Use the 'epoch' as the row index.\n",
    "# df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# # A hack to force the column headers to wrap.\n",
    "# #df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# # Display the table.\n",
    "# df_stats\n",
    "\n",
    "# sns.set(style='darkgrid')\n",
    "\n",
    "# # # Increase the plot size and font size.\n",
    "# # sns.set(font_scale=1.5)\n",
    "# # plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# # Plot the learning curve.\n",
    "# plt.plot(df_stats['training loss'], 'b-o', label=\"Training\")\n",
    "# plt.plot(df_stats['valid loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# # Label the plot.\n",
    "# plt.title(\"Training & Validation Loss\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.legend()\n",
    "# plt.xticks([1, 2])\n",
    "\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
